---
layout: post
title: "[论文理解] Denoising Diffusion Probabilistic Models"
description: "关于 DDPM 的论文理解"

tags: [diffusion, score function, guidance, deep learning, generative models]

image:
  feature: albumsbg.jpg
comments: true
share: false
---



DDPM 样本生成过程缓慢， DDIM 的提出是为了解决 DDPM 样本生成缓慢问题。

## <a name="motivation"></a>动机

DDPM 的采样是根据上一步采样结果 $$\mathbf{x}_t$$ 逐步生成下一步结果 $$\mathbf{x}_{t-1}$$，所以不能跨越步骤，如果扩散过程的时间步为1000，则生成过程的时间步也需要设置为1000，带来的问题是样本生成过程缓慢。

DDIM 的提出是为了解决 DDPM 样本生成缓慢问题，思路是根据 $$\mathbf{x}_t$$ 先预测 $$\mathbf{x}_0$$，然后根据 $$\mathbf{x}_0$$ 再预测 $$\mathbf{x_{t-1}}$$ ，这里可以将 $$\mathbf{x}_0$$ 看作一个跳板，生成过程可以设置任意的时间步，摆脱了 DDPM 时间步的限制。

DDIM 的扩散过程、训练和 DDPM 类似，关于DDPM的详细理解参见前面博客[^1]，主要不同点在于采样过程，两者对比如下图，DDIM 的样本生成过程不再是马尔科夫链，因为 $$\mathbf{x}_{t-1}$$ 不仅依赖 $$\mathbf{x}_t$$ ,也依赖 $$\mathbf{x}_0$$。

<figure align="center">
  <img src="/images/image-20220529205404181.png" >
</figure>


<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图1：DDPM 与 DDIM 采样对比
    </center>
    <br>
</div>


## <a name="paper"></a>扩散过程

DDIM 扩散过程和 DDPM一致，即：

$$q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\mathbf{I}), $$

由上式进一步推导，当已知初始状态$$\mathbf{x_0}$$，可以采样得到任意step $t$ 的 $$\mathbf{x_{t}}$$，上式又可以表示成：

$$q(\mathbf{x}_t \vert \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{{\alpha_t}} \mathbf{x}_0, (1 - {\alpha}_t)\mathbf{I}). $$

实现代码：

```python
def q_xt_x0(x0, t):
  mean = gather(alpha_bar, t) ** 0.5 * x0 # now alpha_bar
  var = 1-gather(alpha_bar, t) # (1-alpha_bar)
  eps = torch.randn_like(x0)
  return mean + (var ** 0.5) * eps
```

<figure align="center">
  <img src="/images/image-20220529215613408.png" >
</figure>


<div>
    <center style="color:#C0C0C0;text-decoration:underline;font-size: 15px">
        图2：扩散过程,在图像中逐步添加噪声
    </center>
    <br>
</div>

